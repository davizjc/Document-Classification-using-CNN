{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6CGRF5Xqx6n",
        "outputId": "a5eb3c3f-e0c1-450b-c019-bd2480b843a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVce6Ezz7QEW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from datetime import date\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-9_9g9o7cWt"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, sizeX:tuple, sizeY:tuple):\n",
        "        super().__init__()\n",
        "        self._sizeX = sizeX  # input\n",
        "        self._sizeY = sizeY  # output\n",
        "        self._model = self._BuildModel()\n",
        "        self._learner = self._BuildLearner()\n",
        "\n",
        "    def call(self, x:tf.Tensor, training:bool=False) -> tf.Tensor:\n",
        "        prediction = self._model(x, training=training)\n",
        "        return prediction\n",
        "\n",
        "    @tf.function\n",
        "    def Train(self, x:tf.Tensor, y:tf.Tensor): #update the model's weights\n",
        "        with tf.GradientTape() as tape:\n",
        "            prediction = self.__call__(x)\n",
        "            loss = self._learner[\"get_loss\"](prediction, y)\n",
        "        gradient = tape.gradient(loss, self._model.trainable_variables)\n",
        "        self._learner[\"optimize\"].apply_gradients(zip(gradient, self._model.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def Validate(self, x:tf.Tensor, y:tf.Tensor) -> tf.Tensor: #evaluating the model's performance\n",
        "        prediction = self.__call__(x, training=False)\n",
        "        review = tf.math.in_top_k(tf.math.argmax(y,axis=1), prediction, 1)\n",
        "        accuracy = tf.math.reduce_mean(tf.cast(review, dtype=\"float32\"))\n",
        "        return accuracy\n",
        "\n",
        "    def _BuildModel(self) -> tf.keras.Model:  # 8 layer doesn't include input layer(else 9)\n",
        "        tensorInput = tf.keras.Input(shape=self._sizeX) #input layer\n",
        "        featureMaps = tf.keras.layers.Lambda(lambda x: x/127.5-1.0)(tensorInput) #This is a Lambda layer that normalizes the input data.\n",
        "        featureMaps = tf.keras.layers.Conv2D(filters=16, kernel_size=[3,3], activation=\"relu\")(featureMaps)\n",
        "        featureMaps = tf.keras.layers.Conv2D(filters=32, kernel_size=[1,3], activation=\"relu\")(featureMaps)\n",
        "        featureMaps = tf.keras.layers.Conv2D(filters=64, kernel_size=[1,3], activation=\"relu\")(featureMaps)\n",
        "        featureMaps = tf.keras.layers.Conv2D(filters=128, kernel_size=[1,3], activation=\"relu\")(featureMaps) #capture more complex patterns in the later stages of the network\n",
        "        featureMaps = tf.keras.layers.GlobalAveragePooling2D()(featureMaps) # applies Global Average Pooling, help reduces the number of parameters and helps to prevent overfitting.\n",
        "        featureMaps = tf.keras.layers.Dropout(0.5)(featureMaps ) #prevent overfitting,\n",
        "\n",
        "        result = tf.keras.layers.Dense(units=self._sizeY[0], activation=\"softmax\")(featureMaps) #output layers, softmax activation function is used to convert the outputs into probability scores for each class, making it suitable for multi-class classification tasks.\n",
        "        model = tf.keras.Model(tensorInput, result) #takes tensorInput as input and gives result as output\n",
        "        return model\n",
        "\n",
        "    def _BuildLearner(self) -> dict:\n",
        "        loser = lambda p, y: tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(p+1e-13),axis=1))\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        learner = {\"get_loss\": loser, \"optimize\": optimizer}\n",
        "        return learner\n",
        "        #creates a loss function\n",
        "        #optimizer (Adam with a learning rate of 1e-3).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AisBg2Iq7v9l"
      },
      "outputs": [],
      "source": [
        "def PrepareDataset(dataDir:str, batchSize:int=10) -> dict:\n",
        "    dataDirDict = {\"train\":pathlib.Path(dataDir)/\"Train\", \"valid\":pathlib.Path(dataDir)/\"Valid\"}\n",
        "\n",
        "    classes = sorted(['passport','national_id_card', 'house_register', 'driver_license'])\n",
        "    classIdxDict = dict(zip(classes,range(len(classes))))\n",
        "\n",
        "    paths = {\"train\":list(), \"valid\":list()}\n",
        "    labels = {\"train\":list(), \"valid\":list()}\n",
        "    for eachSet, eachDir in dataDirDict.items():\n",
        "        for eachPath in eachDir.rglob(\"*\"):\n",
        "            if eachPath.is_file():\n",
        "                paths[eachSet].append(str(eachPath))\n",
        "                labels[eachSet].append(classIdxDict[eachPath.parts[-2]])\n",
        "\n",
        "    dataset = dict()\n",
        "    decoder = lambda x, y: [tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(x), channels=3), [224, 224]), tf.one_hot(y, len(classIdxDict))]\n",
        "    for eachSet in dataDirDict.keys():\n",
        "        eachTFData = tf.data.Dataset.from_tensor_slices((paths[eachSet],labels[eachSet]))\n",
        "        dataset.update({eachSet:eachTFData})\n",
        "        dataset[eachSet] = dataset[eachSet].shuffle(len(paths[eachSet]), reshuffle_each_iteration=True)\n",
        "        dataset[eachSet] = dataset[eachSet].map(decoder, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset[eachSet] = dataset[eachSet].batch(batchSize, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning\n"
      ],
      "metadata": {
        "id": "14JM5ymX-I-n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXPrmJBn7cZF",
        "outputId": "0d1239ce-ce70-470d-9134-d5666b135c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset...\n",
            "Build the CNN model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_internal_grad_fn_186527) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "Epoch: 0,    Train perf: 95.79,    Valid perf: 93.33\n",
            "Epoch: 1,    Train perf: 95.26,    Valid perf: 96.67\n",
            "Epoch: 2,    Train perf: 94.74,    Valid perf: 96.67\n",
            "Epoch: 3,    Train perf: 96.84,    Valid perf: 93.33\n",
            "Epoch: 4,    Train perf: 94.21,    Valid perf: 93.33\n",
            "Epoch: 5,    Train perf: 95.26,    Valid perf: 93.33\n",
            "Epoch: 6,    Train perf: 94.21,    Valid perf: 90.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, restored_function_body while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "savee model\n",
            "Epoch: 7,    Train perf: 97.37,    Valid perf: 96.67\n",
            "Epoch: 8,    Train perf: 97.89,    Valid perf: 93.33\n",
            "Epoch: 9,    Train perf: 92.63,    Valid perf: 90.00\n",
            "Epoch: 10,    Train perf: 95.79,    Valid perf: 93.33\n",
            "Epoch: 11,    Train perf: 95.26,    Valid perf: 93.33\n",
            "Epoch: 12,    Train perf: 96.32,    Valid perf: 96.67\n",
            "Epoch: 13,    Train perf: 93.68,    Valid perf: 93.33\n",
            "Epoch: 14,    Train perf: 97.37,    Valid perf: 93.33\n",
            "Completed!\n"
          ]
        }
      ],
      "source": [
        "if __name__== \"__main__\":\n",
        "    print(\"Preparing dataset...\")\n",
        "    dataset = PrepareDataset(r\"/content/drive/MyDrive/doc\", batchSize=10)\n",
        "    print(\"Build the CNN model...\")\n",
        "    newModel = tf.keras.models.load_model('/content/drive/MyDrive/code/model_up')\n",
        "\n",
        "    print(\"Start training...\")\n",
        "    best_valid_perf = 96\n",
        "    best_train_perf = 96\n",
        "    for epoch in range(15):\n",
        "        perfDict = {\"train\":[], \"valid\":[]}\n",
        "        for inData, outData in dataset[\"train\"]:\n",
        "            newModel.Train(inData, outData)\n",
        "        for inData, outData in dataset[\"train\"]:\n",
        "            perfDict[\"train\"].append(newModel.Validate(inData, outData))\n",
        "        for inData, outData in dataset[\"valid\"]:\n",
        "            perfDict[\"valid\"].append(newModel.Validate(inData, outData))\n",
        "        trainPerf = tf.math.reduce_mean(perfDict[\"train\"]) * 100\n",
        "        validPerf = tf.math.reduce_mean(perfDict[\"valid\"]) * 100\n",
        "        if validPerf > best_valid_perf and trainPerf > best_train_perf:\n",
        "            best_valid_perf = validPerf\n",
        "            best_train_perf = trainPerf\n",
        "            newModel.save('/content/drive/MyDrive/code/model_up',save_format='tf')\n",
        "            print('savee model')\n",
        "        print(f\"Epoch: {epoch},    Train perf: {trainPerf:.2f},    Valid perf: {validPerf:.2f}\")\n",
        "    print(\"Completed!\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6rTta4RKVD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b87aab6-81c2-4f9f-9749-2296850b5015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall train accuracy: 96.84\n",
            "Overall valid accuracy: 96.67\n"
          ]
        }
      ],
      "source": [
        "# Evaluate overall training accuracy\n",
        "train_accuracy = []\n",
        "for inData, outData in dataset[\"train\"]:\n",
        "    train_accuracy.append(newModel.Validate(inData, outData))\n",
        "overall_train_accuracy = tf.reduce_mean(train_accuracy)\n",
        "\n",
        "# Evaluate overall validation accuracy\n",
        "valid_accuracy = []\n",
        "for inData, outData in dataset[\"valid\"]:\n",
        "    valid_accuracy.append(newModel.Validate(inData, outData))\n",
        "overall_valid_accuracy = tf.reduce_mean(valid_accuracy)\n",
        "\n",
        "print(f\"Overall train accuracy: {overall_train_accuracy*100:.2f}\")\n",
        "print(f\"Overall valid accuracy: {overall_valid_accuracy*100:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install db-sqlite3"
      ],
      "metadata": {
        "id": "5mkYMC8gJhYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d1a6dc-e7f7-4eba-9379-c571357b9da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting db-sqlite3\n",
            "  Downloading db-sqlite3-0.0.1.tar.gz (1.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting db (from db-sqlite3)\n",
            "  Downloading db-0.1.1.tar.gz (3.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting antiorm (from db->db-sqlite3)\n",
            "  Downloading antiorm-1.2.1.tar.gz (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: db-sqlite3, db, antiorm\n",
            "  Building wheel for db-sqlite3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for db-sqlite3: filename=db_sqlite3-0.0.1-py3-none-any.whl size=1769 sha256=c5e0215ebfddbda5f56f005c482c9fa15d8201b34c501952468e6e91899b5277\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/b7/83/e941e0a0e04f417982e718ae7295d1e82b5f2863a1c51edd71\n",
            "  Building wheel for db (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for db: filename=db-0.1.1-py3-none-any.whl size=3874 sha256=3775cee5861932b926af5c22e3de1a21b99316f59e7f851dee269686528357a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/e4/df/bc55b93af204ab098d9effec76f6889ad12d7ad74e833c4910\n",
            "  Building wheel for antiorm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antiorm: filename=antiorm-1.2.1-py3-none-any.whl size=31664 sha256=11af27a06a5c0efb2cd631ab36b4c4ca48b5d9358bccc311155433e40a07a2c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/9f/7e/b7c95b391cfa77a9e722d359e9c669cf6c8d798d748aec5091\n",
            "Successfully built db-sqlite3 db antiorm\n",
            "Installing collected packages: antiorm, db, db-sqlite3\n",
            "Successfully installed antiorm-1.2.1 db-0.1.1 db-sqlite3-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image_path, model, class_names, confidence_threshold=0.6):\n",
        "    # Convert image_path to string\n",
        "    image_path = str(image_path)\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [224, 224])\n",
        "    img = img[None, ...]  # Add a batch dimension\n",
        "\n",
        "    # Pass the image through the model and get the predicted probabilities\n",
        "    predictions = model(img)\n",
        "    predicted_class_index = np.argmax(predictions, axis=-1)\n",
        "    predicted_probability = np.max(predictions)\n",
        "    print(predicted_probability)\n",
        "\n",
        "\n",
        "    # Check if the predicted probability is below the threshold\n",
        "    if predicted_probability < confidence_threshold:\n",
        "        return 'other'\n",
        "    else:\n",
        "        return class_names[predicted_class_index[0]]\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "    db_file = 'result.db'\n",
        "\n",
        "    # delete result.db\n",
        "\n",
        "    # if os.path.exists(db_file):\n",
        "    #   os.remove(db_file)\n",
        "\n",
        "    # Connect to the SQLite database or create a new one\n",
        "    conn = sqlite3.connect(db_file)\n",
        "\n",
        "    # Create a cursor object to execute SQL queries\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Create a table for storing file names and results, with file_name being a PRIMARY KEY\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS results (file_name TEXT PRIMARY KEY, result TEXT, date DATE)''')\n",
        "\n",
        "    newModel = tf.keras.models.load_model('/content/drive/MyDrive/code/model_up')  # Load the model\n",
        "    test_dir = \"/content/drive/MyDrive/document/test/driver_license\"\n",
        "    test_images = list(pathlib.Path(test_dir).rglob(\"*\"))\n",
        "    class_names = [ 'driver_license','house_register','national_id_card','passport']\n",
        "\n",
        "    for image_path in test_images:\n",
        "        predicted_class = classify_image(image_path, newModel, class_names)\n",
        "        image_name = os.path.basename(image_path)\n",
        "        print(f\"The image {image_name} is classified as {predicted_class}\")\n",
        "        c.execute(\"INSERT OR REPLACE INTO results VALUES (?, ?, ?)\", (str(image_name), predicted_class, date.today()))\n",
        "        conn.commit()\n",
        "\n",
        "    # Define the SQL query to retrieve all records from the \"results\" table\n",
        "    qry = \"SELECT * FROM results\"\n",
        "\n",
        "    # Read the query result into a DataFrame\n",
        "    df = pd.read_sql_query(qry, conn)\n",
        "\n",
        "    # Close the cursor and connection\n",
        "    c.close()\n",
        "    conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3XWKLkOdqor",
        "outputId": "b11167a0-365c-4ed3-a6b2-d906eb7c87e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Importing a function (__inference_internal_grad_fn_197475) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.99982494\n",
            "The image dl1.jpg is classified as driver_license\n",
            "0.9981687\n",
            "The image dl2.JPG is classified as driver_license\n",
            "0.9989472\n",
            "The image dl3.JPG is classified as driver_license\n",
            "0.66132843\n",
            "The image dl4.JPG is classified as driver_license\n",
            "0.999946\n",
            "The image dl5.JPG is classified as driver_license\n",
            "0.9998022\n",
            "The image dl6.JPG is classified as driver_license\n",
            "0.99721944\n",
            "The image dl7.JPG is classified as driver_license\n",
            "0.99992716\n",
            "The image dl9.JPG is classified as driver_license\n",
            "0.9971193\n",
            "The image dl10.JPG is classified as driver_license\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for image_path in test_images:\n",
        "#     predicted_class = classify_image(image_path, newModel, class_names)\n",
        "#     image_name = os.path.basename(image_path)\n",
        "\n",
        "#     # Load and display the image\n",
        "#     img_display = plt.imread(image_path)\n",
        "#     plt.imshow(img_display)\n",
        "#     plt.title(f\"{image_name} -> {predicted_class}\")\n",
        "#     plt.axis('off')  # Don't show axis values\n",
        "#     plt.show()\n"
      ],
      "metadata": {
        "id": "vBuUnq9L1bbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('result.db')\n",
        "\n",
        "# Define the SQL query\n",
        "qry = \"SELECT * FROM results\"\n",
        "\n",
        "# Read the query result into a DataFrame\n",
        "df = pd.read_sql_query(qry, conn)\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n",
        "\n",
        "# Display the first few records of the DataFrame\n",
        "print(df.head(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BourGH6UK3yy",
        "outputId": "423d0051-4939-459c-ea13-f5731ffc759a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      file_name          result        date\n",
            "0  IMG_0959.JPG           other  2023-07-24\n",
            "1  IMG_0198.JPG  driver_license  2023-07-24\n",
            "2  IMG_0846.JPG           other  2023-07-24\n",
            "3  IMG_1010.JPG        passport  2023-07-24\n",
            "4  IMG_1002.JPG        passport  2023-07-24\n",
            "5  IMG_1007.JPG        passport  2023-07-24\n",
            "6  IMG_0974.JPG        passport  2023-07-24\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}